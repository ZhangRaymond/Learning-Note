{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras Tokenizer 演示"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [分词器 Tokenizer](https://keras-cn.readthedocs.io/en/latest/preprocessing/text/)\n",
    "``` python\n",
    "keras.preprocessing.text.Tokenizer(num_words=None,\n",
    "                                   filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\\t\\n',\n",
    "                                   lower=True,\n",
    "                                   split=\" \",\n",
    "                                   char_level=False)\n",
    "```\n",
    "Tokenizer是一个用于向量化文本，或将文本转换为序列（即单词在字典中的下标构成的列表，从1算起）的类。\n",
    "\n",
    "#### 构造参数\n",
    "- **num_words**：None或整数，处理的最大单词数量。若被设置为整数，则分词器将被限制为待处理数据集中最常见的**num_words**个单词\n",
    "- **char_level**: 如果为 True, 每个字符将被视为一个标记\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = ['Well done!',\n",
    "\t\t'Good work',\n",
    "\t\t'Great effort',\n",
    "\t\t'nice work',\n",
    "\t\t'Excellent!',\n",
    "\t\t'Weak',\n",
    "\t\t'Poor effort!',\n",
    "\t\t'not good',\n",
    "\t\t'poor work',\n",
    "\t\t'Could have done better.']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 建立一个Tokenizer并训练之\n",
    "有两种训练方式：\n",
    " - **fit_on_texts()**\n",
    " - **fit_on_sequences()**   \n",
    " \n",
    "这里主要用到的是前者"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后就可以测试一下各种好用的功能了\n",
    "\n",
    "## 都有些什么功能？\n",
    "### text to XXX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[6, 2],\n",
       " [3, 1],\n",
       " [7, 4],\n",
       " [8, 1],\n",
       " [9],\n",
       " [10],\n",
       " [5, 4],\n",
       " [11, 3],\n",
       " [5, 1],\n",
       " [12, 13, 2, 14]]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.texts_to_sequences(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Tokenizer.texts_to_sequences_generator at 0x000002174D25B678>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.texts_to_sequences_generator(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.texts_to_matrix(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sequences to XXX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['well done',\n",
       " 'good work',\n",
       " 'great effort',\n",
       " 'nice work',\n",
       " 'excellent',\n",
       " 'weak',\n",
       " 'poor effort',\n",
       " 'not good',\n",
       " 'poor work',\n",
       " 'could have done better']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.sequences_to_texts(t.texts_to_sequences(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.sequences_to_matrix(t.texts_to_sequences(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Tokenizer.sequences_to_texts_generator at 0x000002174D25B308>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.sequences_to_texts_generator(t.texts_to_sequences(docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 将其information转换为一个json字符串\n",
    "看看里面都有些啥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'{\"class_name\": \"Tokenizer\", \"config\": {\"num_words\": null, \"filters\": \"!\\\\\"#$%&()*+,-./:;<=>?@[\\\\\\\\]^_`{|}~\\\\t\\\\n\", \"lower\": true, \"split\": \" \", \"char_level\": false, \"oov_token\": null, \"document_count\": 10, \"word_counts\": \"{\\\\\"well\\\\\": 1, \\\\\"done\\\\\": 2, \\\\\"good\\\\\": 2, \\\\\"work\\\\\": 3, \\\\\"great\\\\\": 1, \\\\\"effort\\\\\": 2, \\\\\"nice\\\\\": 1, \\\\\"excellent\\\\\": 1, \\\\\"weak\\\\\": 1, \\\\\"poor\\\\\": 2, \\\\\"not\\\\\": 1, \\\\\"could\\\\\": 1, \\\\\"have\\\\\": 1, \\\\\"better\\\\\": 1}\", \"word_docs\": \"{\\\\\"well\\\\\": 1, \\\\\"done\\\\\": 2, \\\\\"good\\\\\": 2, \\\\\"work\\\\\": 3, \\\\\"effort\\\\\": 2, \\\\\"great\\\\\": 1, \\\\\"nice\\\\\": 1, \\\\\"excellent\\\\\": 1, \\\\\"weak\\\\\": 1, \\\\\"poor\\\\\": 2, \\\\\"not\\\\\": 1, \\\\\"could\\\\\": 1, \\\\\"better\\\\\": 1, \\\\\"have\\\\\": 1}\", \"index_docs\": \"{\\\\\"6\\\\\": 1, \\\\\"2\\\\\": 2, \\\\\"3\\\\\": 2, \\\\\"1\\\\\": 3, \\\\\"4\\\\\": 2, \\\\\"7\\\\\": 1, \\\\\"8\\\\\": 1, \\\\\"9\\\\\": 1, \\\\\"10\\\\\": 1, \\\\\"5\\\\\": 2, \\\\\"11\\\\\": 1, \\\\\"12\\\\\": 1, \\\\\"14\\\\\": 1, \\\\\"13\\\\\": 1}\", \"index_word\": \"{\\\\\"1\\\\\": \\\\\"work\\\\\", \\\\\"2\\\\\": \\\\\"done\\\\\", \\\\\"3\\\\\": \\\\\"good\\\\\", \\\\\"4\\\\\": \\\\\"effort\\\\\", \\\\\"5\\\\\": \\\\\"poor\\\\\", \\\\\"6\\\\\": \\\\\"well\\\\\", \\\\\"7\\\\\": \\\\\"great\\\\\", \\\\\"8\\\\\": \\\\\"nice\\\\\", \\\\\"9\\\\\": \\\\\"excellent\\\\\", \\\\\"10\\\\\": \\\\\"weak\\\\\", \\\\\"11\\\\\": \\\\\"not\\\\\", \\\\\"12\\\\\": \\\\\"could\\\\\", \\\\\"13\\\\\": \\\\\"have\\\\\", \\\\\"14\\\\\": \\\\\"better\\\\\"}\", \"word_index\": \"{\\\\\"work\\\\\": 1, \\\\\"done\\\\\": 2, \\\\\"good\\\\\": 3, \\\\\"effort\\\\\": 4, \\\\\"poor\\\\\": 5, \\\\\"well\\\\\": 6, \\\\\"great\\\\\": 7, \\\\\"nice\\\\\": 8, \\\\\"excellent\\\\\": 9, \\\\\"weak\\\\\": 10, \\\\\"not\\\\\": 11, \\\\\"could\\\\\": 12, \\\\\"have\\\\\": 13, \\\\\"better\\\\\": 14}\"}}'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infor = t.to_json()\n",
    "print(type(infor))  # 是个字符串\n",
    "infor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "直接看字符串实在是阅读不友好，我们将json的字符串格式直接转换为json格式本身"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'class_name': 'Tokenizer',\n",
       " 'config': {'num_words': None,\n",
       "  'filters': '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
       "  'lower': True,\n",
       "  'split': ' ',\n",
       "  'char_level': False,\n",
       "  'oov_token': None,\n",
       "  'document_count': 10,\n",
       "  'word_counts': '{\"well\": 1, \"done\": 2, \"good\": 2, \"work\": 3, \"great\": 1, \"effort\": 2, \"nice\": 1, \"excellent\": 1, \"weak\": 1, \"poor\": 2, \"not\": 1, \"could\": 1, \"have\": 1, \"better\": 1}',\n",
       "  'word_docs': '{\"well\": 1, \"done\": 2, \"good\": 2, \"work\": 3, \"effort\": 2, \"great\": 1, \"nice\": 1, \"excellent\": 1, \"weak\": 1, \"poor\": 2, \"not\": 1, \"could\": 1, \"better\": 1, \"have\": 1}',\n",
       "  'index_docs': '{\"6\": 1, \"2\": 2, \"3\": 2, \"1\": 3, \"4\": 2, \"7\": 1, \"8\": 1, \"9\": 1, \"10\": 1, \"5\": 2, \"11\": 1, \"12\": 1, \"14\": 1, \"13\": 1}',\n",
       "  'index_word': '{\"1\": \"work\", \"2\": \"done\", \"3\": \"good\", \"4\": \"effort\", \"5\": \"poor\", \"6\": \"well\", \"7\": \"great\", \"8\": \"nice\", \"9\": \"excellent\", \"10\": \"weak\", \"11\": \"not\", \"12\": \"could\", \"13\": \"have\", \"14\": \"better\"}',\n",
       "  'word_index': '{\"work\": 1, \"done\": 2, \"good\": 3, \"effort\": 4, \"poor\": 5, \"well\": 6, \"great\": 7, \"nice\": 8, \"excellent\": 9, \"weak\": 10, \"not\": 11, \"could\": 12, \"have\": 13, \"better\": 14}'}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "infor_dic = json.loads(infor)\n",
    "print(type(infor_dic))  # 是个字典\n",
    "infor_dic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "看起来好一些了，还有更好的：发挥jupyter的潜能吧"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "class_name": "Tokenizer",
       "config": {
        "char_level": false,
        "document_count": 10,
        "filters": "!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\t\n",
        "index_docs": "{\"6\": 1, \"2\": 2, \"3\": 2, \"1\": 3, \"4\": 2, \"7\": 1, \"8\": 1, \"9\": 1, \"10\": 1, \"5\": 2, \"11\": 1, \"12\": 1, \"14\": 1, \"13\": 1}",
        "index_word": "{\"1\": \"work\", \"2\": \"done\", \"3\": \"good\", \"4\": \"effort\", \"5\": \"poor\", \"6\": \"well\", \"7\": \"great\", \"8\": \"nice\", \"9\": \"excellent\", \"10\": \"weak\", \"11\": \"not\", \"12\": \"could\", \"13\": \"have\", \"14\": \"better\"}",
        "lower": true,
        "num_words": null,
        "oov_token": null,
        "split": " ",
        "word_counts": "{\"well\": 1, \"done\": 2, \"good\": 2, \"work\": 3, \"great\": 1, \"effort\": 2, \"nice\": 1, \"excellent\": 1, \"weak\": 1, \"poor\": 2, \"not\": 1, \"could\": 1, \"have\": 1, \"better\": 1}",
        "word_docs": "{\"well\": 1, \"done\": 2, \"good\": 2, \"work\": 3, \"effort\": 2, \"great\": 1, \"nice\": 1, \"excellent\": 1, \"weak\": 1, \"poor\": 2, \"not\": 1, \"could\": 1, \"better\": 1, \"have\": 1}",
        "word_index": "{\"work\": 1, \"done\": 2, \"good\": 3, \"effort\": 4, \"poor\": 5, \"well\": 6, \"great\": 7, \"nice\": 8, \"excellent\": 9, \"weak\": 10, \"not\": 11, \"could\": 12, \"have\": 13, \"better\": 14}"
       }
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import JSON\n",
    "JSON(infor_dic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**注意**：不要直接给JSON输入json字符串，它只接受dic和list。否则会抛出warning。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer的config也可以直接获取到"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('well', 1),\n",
       "             ('done', 2),\n",
       "             ('good', 2),\n",
       "             ('work', 3),\n",
       "             ('great', 1),\n",
       "             ('effort', 2),\n",
       "             ('nice', 1),\n",
       "             ('excellent', 1),\n",
       "             ('weak', 1),\n",
       "             ('poor', 2),\n",
       "             ('not', 1),\n",
       "             ('could', 1),\n",
       "             ('have', 1),\n",
       "             ('better', 1)])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.word_counts  # 词频统计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'well': 1,\n",
       "             'done': 2,\n",
       "             'good': 2,\n",
       "             'work': 3,\n",
       "             'effort': 2,\n",
       "             'great': 1,\n",
       "             'nice': 1,\n",
       "             'excellent': 1,\n",
       "             'weak': 1,\n",
       "             'poor': 2,\n",
       "             'not': 1,\n",
       "             'could': 1,\n",
       "             'better': 1,\n",
       "             'have': 1})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.word_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'work': 1,\n",
       " 'done': 2,\n",
       " 'good': 3,\n",
       " 'effort': 4,\n",
       " 'poor': 5,\n",
       " 'well': 6,\n",
       " 'great': 7,\n",
       " 'nice': 8,\n",
       " 'excellent': 9,\n",
       " 'weak': 10,\n",
       " 'not': 11,\n",
       " 'could': 12,\n",
       " 'have': 13,\n",
       " 'better': 14}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'work',\n",
       " 2: 'done',\n",
       " 3: 'good',\n",
       " 4: 'effort',\n",
       " 5: 'poor',\n",
       " 6: 'well',\n",
       " 7: 'great',\n",
       " 8: 'nice',\n",
       " 9: 'excellent',\n",
       " 10: 'weak',\n",
       " 11: 'not',\n",
       " 12: 'could',\n",
       " 13: 'have',\n",
       " 14: 'better'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.index_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' '"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.char_level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.document_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.lower"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## fit_on_sequences()\n",
    "前面使用fit_on_texts()训练的，如果语料是纯数字序列，可以用fit_on_sequences()训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[6, 2], [3, 1], [7, 4], [8, 1], [9]]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seqs = t.texts_to_sequences(docs[:5])  # 得到数字序列\n",
    "seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '', '', '', '']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2 = Tokenizer()\n",
    "t2.fit_on_sequences(seqs)     # 借用t生成的sequence来训练t2\n",
    "t2.sequences_to_texts(seqs)   # 原来就是senquences,肯定拿不到"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "看一下t2的config信息，就都懂了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "class_name": "Tokenizer",
       "config": {
        "char_level": false,
        "document_count": 5,
        "filters": "!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\t\n",
        "index_docs": "{\"2\": 1, \"6\": 1, \"1\": 2, \"3\": 1, \"4\": 1, \"7\": 1, \"8\": 1, \"9\": 1}",
        "index_word": "{}",
        "lower": true,
        "num_words": null,
        "oov_token": null,
        "split": " ",
        "word_counts": "{}",
        "word_docs": "{}",
        "word_index": "{}"
       }
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "execution_count": 23,
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "JSON(json.loads(t2.to_json()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
