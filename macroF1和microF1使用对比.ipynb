{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.metrics import f1_score, fbeta_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = [0, 0, 0, 2, 1, 2, 0, 1, 1, 2]\n",
    "y_pred = [0, 0, 2, 1, 0, 2, 0, 2, 1, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "a1 = accuracy_score(y_true, y_pred) # Return the number of correctly classified samples\n",
    "a2 = accuracy_score(y_true, y_pred, normalize=False) # Return the fraction of correctly classified samples\n",
    "print(a1)\n",
    "print(a2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5833333333333334\n",
      "0.6\n",
      "[0.75 0.5  0.5 ]\n"
     ]
    }
   ],
   "source": [
    "# Calculate precision score\n",
    "p1 = precision_score(y_true, y_pred, average='macro')\n",
    "p2 = precision_score(y_true, y_pred, average='micro')\n",
    "p3 = precision_score(y_true, y_pred, average=None)\n",
    "print(p1)\n",
    "print(p2)\n",
    "print(p3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5833333333333334\n",
      "0.6\n",
      "[0.75       0.33333333 0.66666667]\n"
     ]
    }
   ],
   "source": [
    "# Calculate recall score\n",
    "r1 = recall_score(y_true, y_pred, average='macro')\n",
    "r2 = recall_score(y_true, y_pred, average='micro')\n",
    "r3 = recall_score(y_true, y_pred, average=None)\n",
    "print(r1)\n",
    "print(r2)\n",
    "print(r3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5738095238095239\n",
      "0.6\n",
      "[0.75       0.4        0.57142857]\n"
     ]
    }
   ],
   "source": [
    "# Calculate f1 score\n",
    "f1_1 = f1_score(y_true, y_pred, average='macro')\n",
    "f1_2 = f1_score(y_true, y_pred, average='micro')\n",
    "f1_3 = f1_score(y_true, y_pred, average=None)\n",
    "print(f1_1)\n",
    "print(f1_2)\n",
    "print(f1_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5769537480063796\n",
      "0.6\n",
      "[0.75       0.45454545 0.52631579]\n"
     ]
    }
   ],
   "source": [
    "# Calculate f beta score\n",
    "fb_1 = fbeta_score(y_true, y_pred, average='macro', beta=0.5)\n",
    "fb_2 = fbeta_score(y_true, y_pred, average='micro', beta=0.5)\n",
    "fb_3 = fbeta_score(y_true, y_pred, average=None, beta=0.5)\n",
    "print(fb_1)\n",
    "print(fb_2)\n",
    "print(fb_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
